"""
Week 3: SHAP Analysis and Group-wise Fairness Investigation
ç›®çš„: ã€Œãªãœå·®ãŒå‡ºã‚‹ã‹ã€ã‚’èª¬æ˜ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
"""

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler, LabelEncoder
from xgboost import XGBClassifier
import shap

def preprocess_data(df):
    """
    ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆWeek 2ã¨åŒã˜ï¼‰
    """
    # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯
    required_cols = ['target', 'age_binary', 'sex_binary']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        raise ValueError(f"Missing required columns: {missing_cols}")
    
    # Featuresé™¤å¤–
    exclude_cols = ['target', 'age_binary', 'sex_binary', 'age_group', 'sex_group', 'class']
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    df_features = df[feature_cols].copy()
    numeric_features = df_features.select_dtypes(include=[np.number]).columns.tolist()
    categorical_features = df_features.select_dtypes(include=['object']).columns.tolist()
    
    feature_names = []
    
    if len(categorical_features) > 0:
        for col in categorical_features:
            le = LabelEncoder()
            df_features[col + '_encoded'] = le.fit_transform(df_features[col].astype(str))
            feature_names.append(col)
        
        df_features = df_features.drop(columns=categorical_features)
        df_features.columns = [col.replace('_encoded', '') for col in df_features.columns]
    else:
        feature_names = df_features.columns.tolist()
    
    X = df_features.values
    y = df['target'].values
    age_binary = df['age_binary'].values
    sex_binary = df['sex_binary'].values
    
    return X, y, age_binary, sex_binary, df_features.columns.tolist()

def train_best_model(X_train, y_train):
    """
    XGBoostãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆWeek 2ã¨åŒã˜è¨­å®šï¼‰
    """
    model = XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        eval_metric='logloss',
        use_label_encoder=False
    )
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    model.fit(X_train_scaled, y_train)
    
    return model, scaler

def calculate_shap_values(model, X, feature_names):
    """
    SHAPå€¤ã‚’è¨ˆç®—
    """
    print("\n" + "="*60)
    print("CALCULATING SHAP VALUES")
    print("="*60)
    
    # TreeExplainerï¼ˆXGBoostç”¨ï¼‰
    explainer = shap.TreeExplainer(model)
    
    # SHAPå€¤è¨ˆç®—ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆã¯ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æ¸›ã‚‰ã™ï¼‰
    print(f"\nCalculating SHAP for {X.shape[0]} samples...")
    shap_values = explainer.shap_values(X)
    
    print(f"âœ… SHAP values shape: {shap_values.shape}")
    
    return shap_values, explainer

def create_shap_summary_plot(shap_values, X, feature_names):
    """
    å›³2: SHAPä¸Šä½ç‰¹å¾´ï¼ˆå…¨ä½“ï¼‰
    """
    print("\nğŸ“Š Creating Figure 2: SHAP Summary Plot...")
    
    plt.figure(figsize=(12, 8))
    
    # SHAP summary plot
    shap.summary_plot(
        shap_values, 
        X, 
        feature_names=feature_names,
        max_display=15,
        show=False
    )
    
    plt.title('SHAP Feature Importance (Top 15)', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('SHAP Value (impact on model output)', fontsize=12, fontweight='bold')
    plt.tight_layout()
    plt.savefig('figs/fig2_shap_summary.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… Saved: figs/fig2_shap_summary.png")
    
    # ç‰¹å¾´é‡é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    feature_importance = np.abs(shap_values).mean(axis=0)
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    importance_df.to_csv('results/shap_feature_importance.csv', index=False)
    print("âœ… Saved: results/shap_feature_importance.csv")
    
    return importance_df

def create_shap_bar_plot(shap_values, X, feature_names):
    """
    SHAP bar plotï¼ˆè¿½åŠ å›³ï¼‰
    """
    print("\nğŸ“Š Creating SHAP Bar Plot...")
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’è¨ˆç®—
    feature_importance = np.abs(shap_values).mean(axis=0)
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False).head(15)
    
    # Bar plot
    fig, ax = plt.subplots(figsize=(10, 8))
    
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(importance_df)))
    bars = ax.barh(range(len(importance_df)), importance_df['importance'], color=colors)
    
    ax.set_yticks(range(len(importance_df)))
    ax.set_yticklabels(importance_df['feature'], fontsize=11)
    ax.set_xlabel('Mean |SHAP Value|', fontsize=12, fontweight='bold')
    ax.set_title('Feature Importance (SHAP)', fontsize=14, fontweight='bold')
    ax.invert_yaxis()
    ax.grid(axis='x', alpha=0.3)
    
    # æ•°å€¤ãƒ©ãƒ™ãƒ«
    for i, (bar, val) in enumerate(zip(bars, importance_df['importance'])):
        ax.text(val + 0.001, i, f'{val:.3f}', va='center', fontsize=9)
    
    plt.tight_layout()
    plt.savefig('figs/fig2_shap_bar.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… Saved: figs/fig2_shap_bar.png")

def analyze_group_predictions(model, scaler, X, y, age_binary, sex_binary):
    """
    ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã®ã‚¹ã‚³ã‚¢åˆ†å¸ƒåˆ†æ
    """
    print("\n" + "="*60)
    print("GROUP-WISE PREDICTION ANALYSIS")
    print("="*60)
    
    # äºˆæ¸¬ç¢ºç‡
    X_scaled = scaler.transform(X)
    y_proba = model.predict_proba(X_scaled)[:, 1]
    y_pred = model.predict(X_scaled)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    results_df = pd.DataFrame({
        'true_label': y,
        'predicted_label': y_pred,
        'predicted_proba': y_proba,
        'age_group': ['Young' if a == 1 else 'Old' for a in age_binary],
        'sex_group': ['Female' if s == 1 else 'Male' for s in sex_binary]
    })
    
    # ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥çµ±è¨ˆ
    print("\nğŸ“Š Age Group Statistics:")
    age_stats = results_df.groupby('age_group').agg({
        'predicted_proba': ['mean', 'std', 'min', 'max'],
        'predicted_label': 'mean'
    }).round(4)
    print(age_stats)
    
    print("\nğŸ“Š Sex Group Statistics:")
    sex_stats = results_df.groupby('sex_group').agg({
        'predicted_proba': ['mean', 'std', 'min', 'max'],
        'predicted_label': 'mean'
    }).round(4)
    print(sex_stats)
    
    # TPR/FPRåˆ†æ
    print("\nğŸ“Š TPR/FPR Analysis:")
    
    for attr_name, attr_values in [('Age', age_binary), ('Sex', sex_binary)]:
        print(f"\n{attr_name}:")
        for group_val, group_name in [(0, 'Majority'), (1, 'Minority')]:
            mask = attr_values == group_val
            
            # TPR (True Positive Rate)
            true_positives = ((y[mask] == 1) & (y_pred[mask] == 1)).sum()
            actual_positives = (y[mask] == 1).sum()
            tpr = true_positives / actual_positives if actual_positives > 0 else 0
            
            # FPR (False Positive Rate)
            false_positives = ((y[mask] == 0) & (y_pred[mask] == 1)).sum()
            actual_negatives = (y[mask] == 0).sum()
            fpr = false_positives / actual_negatives if actual_negatives > 0 else 0
            
            print(f"  {group_name}: TPR={tpr:.3f}, FPR={fpr:.3f}")
    
    return results_df

def create_score_distribution_plot(results_df):
    """
    å›³3: ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
    """
    print("\nğŸ“Š Creating Figure 3: Score Distribution by Group...")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Prediction Score Distribution by Protected Groups', 
                 fontsize=16, fontweight='bold')
    
    # Age - Histogram
    ax1 = axes[0, 0]
    for group in ['Young', 'Old']:
        data = results_df[results_df['age_group'] == group]['predicted_proba']
        ax1.hist(data, bins=30, alpha=0.6, label=group, edgecolor='black')
    ax1.set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
    ax1.set_title('Age Groups: Score Distribution', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(alpha=0.3)
    
    # Age - Box plot
    ax2 = axes[0, 1]
    age_data = [
        results_df[results_df['age_group'] == 'Old']['predicted_proba'],
        results_df[results_df['age_group'] == 'Young']['predicted_proba']
    ]
    bp1 = ax2.boxplot(age_data, labels=['Old', 'Young'], patch_artist=True)
    for patch, color in zip(bp1['boxes'], ['skyblue', 'lightcoral']):
        patch.set_facecolor(color)
    ax2.set_ylabel('Predicted Probability', fontsize=12, fontweight='bold')
    ax2.set_title('Age Groups: Score Box Plot', fontsize=14, fontweight='bold')
    ax2.grid(alpha=0.3, axis='y')
    
    # Sex - Histogram
    ax3 = axes[1, 0]
    for group in ['Male', 'Female']:
        data = results_df[results_df['sex_group'] == group]['predicted_proba']
        ax3.hist(data, bins=30, alpha=0.6, label=group, edgecolor='black')
    ax3.set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')
    ax3.set_ylabel('Frequency', fontsize=12, fontweight='bold')
    ax3.set_title('Sex Groups: Score Distribution', fontsize=14, fontweight='bold')
    ax3.legend(fontsize=11)
    ax3.grid(alpha=0.3)
    
    # Sex - Box plot
    ax4 = axes[1, 1]
    sex_data = [
        results_df[results_df['sex_group'] == 'Male']['predicted_proba'],
        results_df[results_df['sex_group'] == 'Female']['predicted_proba']
    ]
    bp2 = ax4.boxplot(sex_data, labels=['Male', 'Female'], patch_artist=True)
    for patch, color in zip(bp2['boxes'], ['lightblue', 'lightpink']):
        patch.set_facecolor(color)
    ax4.set_ylabel('Predicted Probability', fontsize=12, fontweight='bold')
    ax4.set_title('Sex Groups: Score Box Plot', fontsize=14, fontweight='bold')
    ax4.grid(alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('figs/fig3_score_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… Saved: figs/fig3_score_distribution.png")

def create_group_shap_distribution(shap_values, X, feature_names, age_binary, sex_binary, top_n=5):
    """
    å›³3-2: ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥SHAPåˆ†å¸ƒï¼ˆä¸»è¦ç‰¹å¾´ï¼‰
    """
    print("\nğŸ“Š Creating Figure 3-2: Group-wise SHAP Distribution...")
    
    # ä¸Šä½Nç‰¹å¾´ã‚’é¸æŠ
    feature_importance = np.abs(shap_values).mean(axis=0)
    top_indices = np.argsort(feature_importance)[-top_n:][::-1]
    top_features = [feature_names[i] for i in top_indices]
    
    fig, axes = plt.subplots(top_n, 2, figsize=(16, 4*top_n))
    fig.suptitle('SHAP Value Distribution by Protected Groups (Top Features)', 
                 fontsize=16, fontweight='bold')
    
    for idx, (feat_idx, feat_name) in enumerate(zip(top_indices, top_features)):
        feat_shap = shap_values[:, feat_idx]
        
        # Age groups
        ax1 = axes[idx, 0] if top_n > 1 else axes[0]
        young_shap = feat_shap[age_binary == 1]
        old_shap = feat_shap[age_binary == 0]
        
        ax1.hist(old_shap, bins=30, alpha=0.6, label='Old', color='skyblue', edgecolor='black')
        ax1.hist(young_shap, bins=30, alpha=0.6, label='Young', color='lightcoral', edgecolor='black')
        ax1.set_xlabel('SHAP Value', fontsize=11, fontweight='bold')
        ax1.set_ylabel('Frequency', fontsize=11, fontweight='bold')
        ax1.set_title(f'{feat_name} - Age Groups', fontsize=12, fontweight='bold')
        ax1.legend()
        ax1.grid(alpha=0.3)
        ax1.axvline(x=0, color='red', linestyle='--', alpha=0.5)
        
        # Sex groups
        ax2 = axes[idx, 1] if top_n > 1 else axes[1]
        male_shap = feat_shap[sex_binary == 0]
        female_shap = feat_shap[sex_binary == 1]
        
        ax2.hist(male_shap, bins=30, alpha=0.6, label='Male', color='lightblue', edgecolor='black')
        ax2.hist(female_shap, bins=30, alpha=0.6, label='Female', color='lightpink', edgecolor='black')
        ax2.set_xlabel('SHAP Value', fontsize=11, fontweight='bold')
        ax2.set_ylabel('Frequency', fontsize=11, fontweight='bold')
        ax2.set_title(f'{feat_name} - Sex Groups', fontsize=12, fontweight='bold')
        ax2.legend()
        ax2.grid(alpha=0.3)
        ax2.axvline(x=0, color='red', linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    plt.savefig('figs/fig3_group_shap_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… Saved: figs/fig3_group_shap_distribution.png")

def create_shap_dependence_plots(shap_values, X, feature_names, top_n=5):
    """
    SHAP dependence plotsï¼ˆè¿½åŠ åˆ†æï¼‰
    """
    print("\nğŸ“Š Creating SHAP Dependence Plots...")
    
    # ä¸Šä½ç‰¹å¾´ã‚’é¸æŠ
    feature_importance = np.abs(shap_values).mean(axis=0)
    top_indices = np.argsort(feature_importance)[-top_n:][::-1]
    
    fig, axes = plt.subplots(1, top_n, figsize=(5*top_n, 4))
    fig.suptitle('SHAP Dependence Plots (Top Features)', fontsize=16, fontweight='bold')
    
    for idx, feat_idx in enumerate(top_indices):
        ax = axes[idx] if top_n > 1 else axes
        
        shap.dependence_plot(
            feat_idx,
            shap_values,
            X,
            feature_names=feature_names,
            ax=ax,
            show=False
        )
        ax.set_title(feature_names[feat_idx], fontsize=12, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('figs/shap_dependence_plots.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… Saved: figs/shap_dependence_plots.png")

def create_bias_hypothesis_report(importance_df, results_df):
    """
    ãƒã‚¤ã‚¢ã‚¹è¦å› ã®ä»®èª¬ãƒ¡ãƒ¢ã‚’ç”Ÿæˆ
    """
    print("\n" + "="*60)
    print("CREATING BIAS HYPOTHESIS REPORT")
    print("="*60)
    
    report = []
    
    report.append("# Week 3: ãƒã‚¤ã‚¢ã‚¹è¦å› ã®ä»®èª¬åˆ†æ")
    report.append("")
    report.append("## åˆ†ææ—¥: 2026å¹´2æœˆXXæ—¥")
    report.append("")
    
    # ä¸Šä½ç‰¹å¾´é‡
    report.append("## 1. é‡è¦ç‰¹å¾´é‡ï¼ˆä¸Šä½10ï¼‰")
    report.append("")
    report.append("| é †ä½ | ç‰¹å¾´é‡ | SHAPé‡è¦åº¦ |")
    report.append("|------|--------|------------|")
    for i, row in importance_df.head(10).iterrows():
        report.append(f"| {i+1} | {row['feature']} | {row['importance']:.4f} |")
    report.append("")
    
    # ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥çµ±è¨ˆ
    report.append("## 2. ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥äºˆæ¸¬çµ±è¨ˆ")
    report.append("")
    report.append("### å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—")
    report.append("")
    age_stats = results_df.groupby('age_group')['predicted_proba'].agg(['mean', 'std'])
    report.append("| ã‚°ãƒ«ãƒ¼ãƒ— | å¹³å‡ç¢ºç‡ | æ¨™æº–åå·® |")
    report.append("|----------|----------|----------|")
    for group in age_stats.index:
        mean_val = age_stats.loc[group, 'mean']
        std_val = age_stats.loc[group, 'std']
        report.append(f"| {group} | {mean_val:.4f} | {std_val:.4f} |")
    report.append("")
    
    report.append("### æ€§åˆ¥ã‚°ãƒ«ãƒ¼ãƒ—")
    report.append("")
    sex_stats = results_df.groupby('sex_group')['predicted_proba'].agg(['mean', 'std'])
    report.append("| ã‚°ãƒ«ãƒ¼ãƒ— | å¹³å‡ç¢ºç‡ | æ¨™æº–åå·® |")
    report.append("|----------|----------|----------|")
    for group in sex_stats.index:
        mean_val = sex_stats.loc[group, 'mean']
        std_val = sex_stats.loc[group, 'std']
        report.append(f"| {group} | {mean_val:.4f} | {std_val:.4f} |")
    report.append("")
    
    # ä»®èª¬
    report.append("## 3. ãƒã‚¤ã‚¢ã‚¹è¦å› ã®ä»®èª¬ï¼ˆæ–­å®šã—ãªã„è¡¨ç¾ï¼‰")
    report.append("")
    report.append("### ä»®èª¬1: ä»£ç†å¤‰æ•°ã«ã‚ˆã‚‹ç·©å’Œ")
    report.append("")
    report.append("**è¦³å¯Ÿ:**")
    report.append("- ä¸Šä½ç‰¹å¾´ã«é›‡ç”¨æœŸé–“ã€è²¯è“„é¡ãªã©ãŒå«ã¾ã‚Œã‚‹")
    report.append("- ã“ã‚Œã‚‰ã¯å¹´é½¢ãƒ»æ€§åˆ¥ã¨ç›¸é–¢ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹")
    report.append("")
    report.append("**è€ƒãˆã‚‰ã‚Œã‚‹èª¬æ˜:**")
    report.append("- é›‡ç”¨æœŸé–“ãŒé•·ã„ â†’ ä¿¡ç”¨åº¦é«˜ã„ï¼ˆå¹´é½¢ã¨ç›¸é–¢ï¼‰")
    report.append("- è²¯è“„é¡ãŒå¤šã„ â†’ ä¿¡ç”¨åº¦é«˜ã„ï¼ˆå¹´é½¢ãƒ»æ€§åˆ¥ã¨ç›¸é–¢ï¼‰")
    report.append("- ãƒ¢ãƒ‡ãƒ«ãŒå¹´é½¢ãƒ»æ€§åˆ¥ã‚ˆã‚Šã€Œå®Ÿè³ªçš„ãªä¿¡ç”¨æŒ‡æ¨™ã€ã‚’é‡è¦–")
    report.append("- çµæœã¨ã—ã¦ã€ç›´æ¥çš„ãªãƒã‚¤ã‚¢ã‚¹ãŒç·©å’Œã•ã‚Œã‚‹")
    report.append("")
    report.append("**æ³¨æ„:** ä»£ç†å¤‰æ•°ãŒå¿…ãšã—ã‚‚å› æœé–¢ä¿‚ã‚’ç¤ºã™ã¨ã¯é™ã‚‰ãªã„")
    report.append("")
    
    report.append("### ä»®èª¬2: ãƒ‡ãƒ¼ã‚¿ã®è³ª")
    report.append("")
    report.append("**è¦³å¯Ÿ:**")
    report.append("- German Credit Dataã¯1990å¹´ä»£ã®ãƒ‰ã‚¤ãƒ„ã®ãƒ‡ãƒ¼ã‚¿")
    report.append("- å…ƒãƒ‡ãƒ¼ã‚¿ã§ã®ã‚°ãƒ«ãƒ¼ãƒ—é–“å·®ãŒå°ã•ã„")
    report.append("")
    report.append("**è€ƒãˆã‚‰ã‚Œã‚‹èª¬æ˜:**")
    report.append("- å½“æ™‚ã®ãƒ‰ã‚¤ãƒ„ã®ä¸ä¿¡å¯©æŸ»ãŒæ¯”è¼ƒçš„å…¬å¹³ã ã£ãŸå¯èƒ½æ€§")
    report.append("- ãƒ‡ãƒ¼ã‚¿åé›†æ™‚ã«æ—¢ã«ä¸€å®šã®å…¬å¹³æ€§é…æ…®ãŒã‚ã£ãŸå¯èƒ½æ€§")
    report.append("- ã‚µãƒ³ãƒ—ãƒ«é¸æŠãƒã‚¤ã‚¢ã‚¹ã®å¯èƒ½æ€§ï¼ˆå…¬å¹³ãªã‚±ãƒ¼ã‚¹ã®ã¿åéŒ²ï¼‰")
    report.append("")
    report.append("**æ³¨æ„:** æ­´å²çš„èƒŒæ™¯ã®æ¤œè¨¼ãŒå¿…è¦")
    report.append("")
    
    report.append("### ä»®èª¬3: ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘åº¦")
    report.append("")
    report.append("**è¦³å¯Ÿ:**")
    report.append("- XGBoostã®è¨­å®š: max_depth=6, n_estimators=100")
    report.append("- 1000ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦é©åˆ‡ãªè¤‡é›‘åº¦")
    report.append("")
    report.append("**è€ƒãˆã‚‰ã‚Œã‚‹èª¬æ˜:**")
    report.append("- éå­¦ç¿’ã—ã¦ã„ãªã„ãŸã‚ã€åã£ãŸç›¸äº’ä½œç”¨ã‚’å­¦ç¿’ã—ãªã„")
    report.append("- æ­£å‰‡åŒ–ã«ã‚ˆã‚Šã€ä¿è­·å±æ€§ã¸ã®éåº¦ãªä¾å­˜ãŒæŠ‘åˆ¶")
    report.append("- çµæœã¨ã—ã¦ã€ãƒã‚¤ã‚¢ã‚¹ãŒå°ã•ããªã‚‹")
    report.append("")
    report.append("**æ³¨æ„:** ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ç•°ãªã‚‹çµæœã®å¯èƒ½æ€§")
    report.append("")
    
    report.append("## 4. ä»Šå¾Œã®æ¤œè¨¼èª²é¡Œ")
    report.append("")
    report.append("1. **ç‰¹å¾´é‡ç›¸é–¢åˆ†æ**")
    report.append("   - å¹´é½¢ vs é›‡ç”¨æœŸé–“ã®ç›¸é–¢ä¿‚æ•°")
    report.append("   - æ€§åˆ¥ vs è²¯è“„é¡ã®ç›¸é–¢ä¿‚æ•°")
    report.append("")
    report.append("2. **Ablation Study**")
    report.append("   - ä¸Šä½ç‰¹å¾´ã‚’é™¤å¤–ã—ãŸå ´åˆã®ãƒã‚¤ã‚¢ã‚¹å¤‰åŒ–")
    report.append("   - ä¿è­·å±æ€§ã‚’ç›´æ¥å«ã‚ãŸå ´åˆã®æ¯”è¼ƒ")
    report.append("")
    report.append("3. **ä»–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ¤œè¨¼**")
    report.append("   - Adult Income Datasetãªã©")
    report.append("   - ãƒã‚¤ã‚¢ã‚¹ãŒå¤§ãã„ãƒ‡ãƒ¼ã‚¿ã§ã‚‚åŒæ§˜ã®å‚¾å‘ã‹ï¼Ÿ")
    report.append("")
    
    report.append("## 5. çµè«–ï¼ˆæš«å®šï¼‰")
    report.append("")
    report.append("æœ¬åˆ†æã§ã¯ã€ä»¥ä¸‹ã®å¯èƒ½æ€§ãŒç¤ºå”†ã•ã‚ŒãŸï¼š")
    report.append("")
    report.append("1. âœ… ä»£ç†å¤‰æ•°ï¼ˆé›‡ç”¨æœŸé–“ã€è²¯è“„é¡ãªã©ï¼‰ãŒä¿è­·å±æ€§ã®æƒ…å ±ã‚’")
    report.append("   é©åˆ‡ã«ä»£æ›¿ã—ã€ç›´æ¥çš„ãªãƒã‚¤ã‚¢ã‚¹ã‚’ç·©å’Œã—ã¦ã„ã‚‹ **å¯èƒ½æ€§**")
    report.append("")
    report.append("2. âœ… ãƒ‡ãƒ¼ã‚¿ã®è³ªãŒé«˜ãã€å…ƒã€…ãƒã‚¤ã‚¢ã‚¹ãŒå°ã•ã„ **å¯èƒ½æ€§**")
    report.append("")
    report.append("3. âœ… ãƒ¢ãƒ‡ãƒ«ã®é©åˆ‡ãªè¤‡é›‘åº¦ãŒéå­¦ç¿’ã‚’é˜²ãã€ãƒã‚¤ã‚¢ã‚¹ã‚’")
    report.append("   æŠ‘åˆ¶ã—ã¦ã„ã‚‹ **å¯èƒ½æ€§**")
    report.append("")
    report.append("**é‡è¦:** ã“ã‚Œã‚‰ã¯ä»®èª¬ã§ã‚ã‚Šã€æ›´ãªã‚‹æ¤œè¨¼ãŒå¿…è¦ã§ã‚ã‚‹ã€‚")
    report.append("æ–­å®šçš„ãªå› æœé–¢ä¿‚ã‚’ä¸»å¼µã™ã‚‹ã«ã¯ã€è¿½åŠ ã®å®Ÿé¨“ã¨åˆ†æãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã€‚")
    report.append("")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open('results/bias_hypothesis_report.md', 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    print("âœ… Saved: results/bias_hypothesis_report.md")

def main():
    print("="*60)
    print("WEEK 3: SHAP ANALYSIS AND GROUP-WISE FAIRNESS")
    print("="*60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“¥ Loading data...")
    df = pd.read_csv('data/german_credit_processed.csv')
    print(f"âœ… Loaded {len(df)} samples")
    
    # å‰å‡¦ç†
    print("\nğŸ”§ Preprocessing...")
    X, y, age_binary, sex_binary, feature_names = preprocess_data(df)
    print(f"âœ… Features: {len(feature_names)}")
    print(f"âœ… Feature names: {feature_names[:5]}... (showing first 5)")
    
    # Train/Test splitï¼ˆWeek 2ã¨åŒã˜ï¼‰
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test, age_train, age_test, sex_train, sex_test = train_test_split(
        X, y, age_binary, sex_binary, test_size=0.3, random_state=42, stratify=y
    )
    
    print(f"\nâœ… Train size: {len(X_train)}, Test size: {len(X_test)}")
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    print("\nğŸ¯ Training XGBoost model...")
    model, scaler = train_best_model(X_train, y_train)
    print("âœ… Model trained")
    
    # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§SHAPåˆ†æ
    X_test_scaled = scaler.transform(X_test)
    
    # SHAPå€¤è¨ˆç®—
    shap_values, explainer = calculate_shap_values(model, X_test_scaled, feature_names)
    
    # å›³2: SHAP summary
    importance_df = create_shap_summary_plot(shap_values, X_test_scaled, feature_names)
    
    # SHAP bar plot
    create_shap_bar_plot(shap_values, X_test_scaled, feature_names)
    
    # SHAP dependence plots
    create_shap_dependence_plots(shap_values, X_test_scaled, feature_names, top_n=5)
    
    # ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥åˆ†æ
    results_df = analyze_group_predictions(model, scaler, X_test, y_test, age_test, sex_test)
    
    # å›³3: ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
    create_score_distribution_plot(results_df)
    
    # å›³3-2: ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥SHAPåˆ†å¸ƒ
    create_group_shap_distribution(shap_values, X_test_scaled, feature_names, age_test, sex_test, top_n=5)
    
    # ä»®èª¬ãƒ¬ãƒãƒ¼ãƒˆ
    create_bias_hypothesis_report(importance_df, results_df)
    
    print("\n" + "="*60)
    print("âœ… WEEK 3 ANALYSIS COMPLETED!")
    print("="*60)
    print("\næˆæœç‰©:")
    print("  - figs/fig2_shap_summary.png")
    print("  - figs/fig2_shap_bar.png")
    print("  - figs/fig3_score_distribution.png")
    print("  - figs/fig3_group_shap_distribution.png")
    print("  - figs/shap_dependence_plots.png")
    print("  - results/shap_feature_importance.csv")
    print("  - results/bias_hypothesis_report.md")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. å›³2ãƒ»å›³3ã‚’ãƒã‚¹ã‚¿ãƒ¼ã«ä½¿ç”¨")
    print("  2. ä»®èª¬ãƒ¬ãƒãƒ¼ãƒˆã‚’ç²¾æŸ»")
    print("  3. Week 4ã§ãƒã‚¹ã‚¿ãƒ¼ä½œæˆ")

if __name__ == "__main__":
    main()
